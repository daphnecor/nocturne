{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nocturne concepts\n",
    "\n",
    "This page introduces the most basic elements of nocturne. You can find further information about these [in Section 3 of the paper](https://arxiv.org/abs/2206.09889).\n",
    "\n",
    "_Last update: April 2023_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nocturne import Simulation\n",
    "\n",
    "data_path = 'data/example_scenario.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Nocturne simulations are discretized traffic scenarios. A scenario is a constructed snapshot of traffic situation at a particular timepoint.\n",
    "- The state of the vehicle of focus is referred to as the ego state. Each vehicles observes the traffic scene from their own viewpoint and a visible state is constructed by parameterizing the view distance, head angle and cone radius of the driver. The action for each vehicle is a `(1, 3)` tuple with the acceleration, steering and head angle of the vehicle.\n",
    "- The step method advances the simulation with a desired step size. By default, the dynamics of vehicles are driven by a kinematic bicycle model. If a vehicle is set to expert-controlled mode, its position, heading, and speed will be updated according to a trajectory created by a human expert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "In Nocturne, a simulation discretizes an existing traffic scenario. At the moment, Nocturne supports traffic scenarios from the Waymo Open Dataset, but can be further extended to work with other driving datasets.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1nv5Rbyf7ZfdqTdaUduXvEI7ncdkLpDjc' width=650'/>\n",
    "<figcaption></figcaption>An example of a set of traffic scenario's in Nocturne. Upon initialization, a start time is chosen. After each iteration we take a step in the simulation, which gets us to the next scenario. This is done until we reach the end of the simulation. </center>\n",
    "</figure>\n",
    "\n",
    "We show an example of this using `example_scenario.json`, where our traffic data is extracted from the Waymo open motion dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_config = {\n",
    "    'start_time': 0,  # When to start the simulation\n",
    "    'allow_non_vehicles': True,  # Whether to include cyclists and pedestrians\n",
    "    'max_visible_road_points': 10,  # Maximum number of road points for a vehicle\n",
    "    'max_visible_objects': 10,  # Maximum number of road objects for a vehicle\n",
    "    'max_visible_traffic_lights': 10,  # Maximum number of traffic lights in constructed view\n",
    "    'max_visible_stop_signs': 10,  # Maximum number of stop signs in constructed view\n",
    "}\n",
    "\n",
    "# Create simulation\n",
    "sim = Simulation(data_path, scenario_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "\n",
    "A simulation consists of a set of scenarios. A scenario is a snapshot of the traffic scene at a particular timepoint.\n",
    "\n",
    "Here is how to create a scenario object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get traffic scenario at timepoint\n",
    "scenario = sim.getScenario()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scenario` objects holds information we are interested in. Here are a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of road objects in the scene\n",
    "len(scenario.getObjects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The road objects that moved at a particular timepoint\n",
    "objects_that_moved = scenario.getObjectsThatMoved()\n",
    "\n",
    "print(f'Total # moving objects: {len(objects_that_moved)}\\n')\n",
    "print(f'Object IDs of moving vehicles: \\n {[obj.getID() for obj in objects_that_moved]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of road lines\n",
    "len(scenario.road_lines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.getVehicles()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No cyclists in this scene\n",
    "scenario.getCyclists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all moving vehicles that move\n",
    "moving_vehicles = [obj for obj in scenario.getVehicles() if obj in objects_that_moved]\n",
    "\n",
    "print(f'Found {len(moving_vehicles)} moving vehicles in scene: {[vehicle.getID() for vehicle in moving_vehicles]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ego state\n",
    "\n",
    "The **ego state** is an array with features that describe the current vehicle. This array holds the following information:\n",
    "- 0: length of ego vehicle\n",
    "- 1: width of ego vehicle\n",
    "- 2: speed of ego vehicle\n",
    "- 3: distance to the goal position of ego vehicle\n",
    "- 4: angle to the goal (target azimuth)\n",
    "- 5: desired heading at goal position\n",
    "- 6: desired speed at goal position\n",
    "- 7: current acceleration\n",
    "- 8: current steering position\n",
    "- 9: current head angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an arbitrary vehicle\n",
    "ego_vehicle = moving_vehicles[0]\n",
    "\n",
    "print(f'Selected vehicle # {ego_vehicle.getID()}')\n",
    "\n",
    "# Get the state for ego vehicle\n",
    "scenario.ego_state(ego_vehicle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visible state\n",
    "\n",
    "We use the ego vehicle state, together with a view distance (how far the vehicle can see) and a view angle to construct the **visible state**. The figure below shows this procedure for a simplified traffic scene.\n",
    "\n",
    "Calling `scenario.visible_state()` returns a dictionary with four matrices:\n",
    "- `stop_signs`: The visible stop signs\n",
    "- `traffic_lights`: The states for the traffic lights from the perspective of the ego driver(red, yellow, green).\n",
    "- `road_points`: The observable road points (static elements in the scene).\n",
    "- `objects`: The observable road objects (vehicles, pedestrians and cyclists).\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1fG43NvPCzaimmW99asRdB73qY-F4u-q0' width='700'/>\n",
    "<figcaption>To investigate coordination under partial observability, agents in Nocturne can only see an obstructed view of their environment. In this simplified traffic scene, we construct the state for the red ego driver. Note that Nocturne assumes that stop signs can be viewed, even if they are behind another driver. </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\\begin{align*}\n",
    "\\end{align*}\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1egNkFArE-n4cp6KbeoQyWeePiQ28jYYE' width='300'/>\n",
    "<figcaption>The same scene, this time showing the view of the yellow car.</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the visible state is a function of the maximum number of visible objects defined at initialization (traffic lights, stop signs, road objects, and road points) and whether we add padding. If `padding = True`, an array is of size `(max visible objects,  # features)` is always constructed, even if there are no visible objects. Otherwise, if `padding = False` new entries are only created when objects are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define viewing distance, radius and head angle\n",
    "view_distance = 80\n",
    "view_angle = np.radians(120)\n",
    "head_angle = 0\n",
    "padding = True\n",
    "\n",
    "# Construct the visible state for ego vehicle\n",
    "visible_state = scenario.visible_state(\n",
    "    ego_vehicle,\n",
    "    view_dist=view_distance,\n",
    "    view_angle=view_angle,\n",
    "    head_angle=head_angle,\n",
    "    padding=padding,\n",
    ")\n",
    "\n",
    "visible_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no visible stop signs at this point\n",
    "visible_state['stop_signs'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic light states are filtered out in this version of Nocturne\n",
    "visible_state['traffic_lights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max visible road points x 13 features\n",
    "visible_state['road_points'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of visible road objects x 13 features\n",
    "visible_state['objects'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_state_dim = sum([val.flatten().shape[0] for key, val in visible_state.items()])\n",
    "\n",
    "print(f'Dimension flattened visible state: {visible_state_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also flatten the visible state\n",
    "# flattened has padding: if we miss an object --> zeros\n",
    "visible_state_flat = scenario.flattened_visible_state(\n",
    "        ego_vehicle,\n",
    "        view_dist=view_distance,\n",
    "        view_angle=view_angle,\n",
    "        head_angle=head_angle,\n",
    ")\n",
    "\n",
    "visible_state_flat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `.flattened_visible_state()` has padding by default."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step\n",
    "\n",
    "`step(dt)` is a method call on an instance of the Simulation class, where `dt` is a scalar that represents the length of each simulation timestep in seconds. It advances the simulation by one timestep, which can result in changes to the state of the simulation (for example, new positions of objects, updated velocities, etc.) based on the physical laws and rules defined in the simulation.\n",
    "\n",
    "In the Waymo dataset, the length of the expert data is 9 seconds, a step size of 0.1 is used to discretize each traffic scene. The first second is used as a warm-start, leaving the remaining 8 seconds (80 steps) for the simulation (Details in Section 3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "\n",
    "# Step the simulation\n",
    "sim.step(dt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle control\n",
    "\n",
    "By default, vehicles in Nocturne are driven by a **kinematic bicycle model**. This means that calling the `step(dt)` method evolves the dynamics of a vehicle according to the following set of equations (Appendix D in the paper):\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{position: } x_{t+1} &= x_t + \\dot{x} \\, \\Delta t \\\\\n",
    "    y_{t+1} &= y_t + \\dot{y} \\, \\Delta t \\\\\n",
    "    \\textbf{heading: } \\theta_{t+1} &= \\theta_t + \\dot{\\theta} \\, \\Delta t \\\\\n",
    "    \\textbf{speed: } v_{t+1} &= \\text{clip}(v_t + \\dot{v} \\, \\Delta t, -v_{\\text{max}}, v_{\\text{max}}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dot{v} &= a \\\\\n",
    "    \\bar{v} &= \\text{clip}(v_t, + 0.5 \\, \\dot{v} \\, \\Delta \\, t ,\\, - v_{\\text{max}}, v_{\\text{max}}) \\\\\n",
    "    \\beta &= \\tan^{-1} \\left( \\frac{l_r \\tan (\\delta)}{L}  \\right) \\\\\n",
    "          &= \\tan^{-1} (0.5 \\tan(\\delta)) \\\\\n",
    "    \\dot{x} &= \\bar{v} \\cos (\\theta + \\beta) \\\\\n",
    "    \\dot{y} &= \\bar{v} \\sin (\\theta + \\beta) \\\\\n",
    "    \\dot{\\theta} &= \\frac{\\bar{v} \\cos (\\beta)\\tan(\\delta)}{L}\n",
    "\\end{align*}\n",
    "\n",
    "where $(x_t, y_t)$ is the position of a vehicle at time $t$, $\\theta_t$ is the vehicles heading angle, $a$ is the acceleration and $\\delta$ is the steering angle. Finally, $L$ is the length of the car and $l_r = 0.5L$ is the distance to the rear axle of the car.\n",
    "\n",
    "If we set a vehicle to be **expert-controlled** instead, it will follow the same path as the respective human driver. This means that when we call the `step(dt)` function, the vehicle's position, heading, and speed will be updated to match the next point in the recorded human trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, all vehicles are not expert controlled\n",
    "ego_vehicle.expert_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a vehicle to be expert controlled:\n",
    "ego_vehicle.expert_control = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Pseudocode**: How `step(dt)` advances the simulation for every vehicle. Full code is implemented in [scenario.cc](https://github.com/facebookresearch/nocturne/blob/ae0a4e361457caf6b7e397675cc86f46161405ed/nocturne/cpp/src/scenario.cc#L264)\n",
    "\n",
    "---\n",
    "\n",
    "```Python\n",
    "for vehicle in vehicles:\n",
    "\n",
    "    if object is not expert controlled:\n",
    "        step vehicle dynamics following the kinematic bicycle model\n",
    "\n",
    "    if vehicle is expert controlled:\n",
    "        get current time & vehicle idx\n",
    "        vehicle position = expert trajectories[vehicle_idx, time]\n",
    "        vehicle heading = expert headings[vehicle_idx, time]\n",
    "        vehicle speed = expert speeds[vehicle_idx, time]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "The action set for a vehicle consists of three components: acceleration, steering and the head angle. Actions are discretized based on a provided upper and lower bound.\n",
    "\n",
    "The experiments in the paper use:\n",
    "- 6 discrete actions for **acceleration** uniformly split between $[-3, 2] \\, \\frac{m}{s^2}$\n",
    "- 21 discrete actions for **steering** between $[-0.7, 0.7]$ radians\n",
    "- 5 discrete actions for **head tilt** between $[-1.6, 1.6]$ radians\n",
    "\n",
    "This is how you can access an expert action for a vehicle in Nocturne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an arbitrary timepoint\n",
    "time = 5\n",
    "\n",
    "# Show expert action at timepoint\n",
    "scenario.expert_action(ego_vehicle, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scenario.expert_action(ego_vehicle, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did the vehicle's position change after taking this action?\n",
    "scenario.expert_pos_shift(ego_vehicle, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did the head angle change?\n",
    "scenario.expert_heading_shift(ego_vehicle, time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nocturne-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
